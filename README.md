# Solution for Malicious Login Detection 2024 Competition by NovaceneAI (First Prize)

The task involved developing a model capable of detecting malicious activity within a system using the BETH dataset, introduced in the paper "BETH Dataset: Real Cybersecurity Data for Anomaly Detection Research" by Kate Highnam et al. This dataset contains high-quality cybersecurity data and was specifically designed for anomaly detection and out-of-distribution analysis. To tackle this challenge, I employed a one-class SVM with a quantum kernel implemented using an 8-qubit circuit. To enable faster training and inference, I applied the Nyström method to approximate the feature map using only a subset of the data. Using an ideal, noiseless simulator, my model achieved a higher F1-score than a classical SVM with the radial basis function (RBF) kernel.

More on the competition: [https://aqora.io/competitions/novaceneai-beth](https://aqora.io/competitions/novaceneai-beth)

# Repository Structure and How to Run

The `Submission` folder contains the `train` and `run` scripts, along with the artifacts generated by the training script that are needed for execution. The scripts assume the Aqora environment for this challenge is set up. To set up the environment, you can follow the general Aqora **[tutorial](https://aqora.io/competitions/h2-groundstate-energy/data)**, using the specific template name for this competition found **[here](https://aqora.io/competitions/novaceneai-beth/data)**.

The notebook demonstrates the solution process, although the cell outputs may not reflect the final, optimized result. To run the notebook (e.g., in Google Colab), you'll need to download the **[BETH Dataset](https://www.kaggle.com/datasets/katehighnam/beth-dataset/code)** from Kaggle and place it in a Google Drive folder.

# Solution Overview

## Summary

I used a quantum kernel with a One-Class Support Vector Machine (SVM). For faster processing, I approximated the quantum feature map using the **[Nyström method](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.Nystroem.html)** and trained the model with Stochastic Gradient Descent.

## Data Preprocessing

Following the preprocessing steps outlined in the BETH dataset paper by Highnam et al., I applied these transformations:

1.  **Dropped 11 columns:** `timestamp`, `processId`, `threadId`, `parentProcessId`, `userId`, `mountNamespace`, `processName`, `hostName`, `eventName`, `stackAddresses`, and `args`.
2.  **Replaced `processId`** with a binary variable indicating if the ID is 0, 1, or 2. The same was done for `parentProcessId`.
3.  **Replaced `userId`** with a binary variable indicating if the ID is less than 1000.
4.  **Replaced `mountNamespace`** with a binary variable indicating if the namespace is `4026531840`.
5.  **Mapped `returnValue`** to three values: -1 for negative (errors), 0 for zero (success), and 1 for positive (success with signaling).

Afterward, I applied the following additional steps:

1.  **Removed rows with 'sus' occurrences** from the training data, as the one-class SVM is designed to learn a boundary around normal, non-suspicious data.
2.  **Applied Min-Max scaling** to the `argsNum` column.
3.  **Performed frequency encoding** on `eventId`, mapping categorical values to `$n/N$`, where `$n$` is the number of occurrences of a value and `$N$` is the total number of rows.

## Quantum Kernel

As the quantum feature map, I used Qiskit's **[`ZZFeatureMap`](https://www.google.com/search?q=%5Bhttps://docs.quantum.ibm.com/api/qiskit/qiskit.circuit.library.ZZFeatureMap%5D(https://docs.quantum.ibm.com/api/qiskit/qiskit.circuit.library.ZZFeatureMap))** with 2 layers and linear entanglement. The quantum kernel was then computed based on the fidelity between the mapped data points, representing the similarity between quantum states. I calculated this fidelity using the **[`ComputeUncompute`](https://www.google.com/search?q=%5Bhttps://docs.quantum.ibm.com/api/qiskit/qiskit.algorithms.state_fidelities.ComputeUncompute%5D(https://docs.quantum.ibm.com/api/qiskit/qiskit.algorithms.state_fidelities.ComputeUncompute))** method.

## Feature Map Approximation

Directly computing the kernel matrix with a quantum kernel is computationally expensive and restricted our training set to ~100 samples. To mitigate this, I employed the **[Nyström algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.Nystroem.html)** to approximate the quantum feature map, using `n_components=100`. This significantly reduced the number of required circuit simulations and allowed the training set to be increased to 8,000 samples.

## Training

Once the transformed dataset was obtained from the approximate feature map, I trained a linear One-Class SVM using Stochastic Gradient Descent over 31 epochs. The model was trained using the **[`SGDOneClassSVM`](https://www.google.com/search?q=%5Bhttps://scikit-learn.org/dev/modules/generated/sklearn.linear_model.SGDOneClassSVM.html%5D(https://scikit-learn.org/dev/modules/generated/sklearn.linear_model.SGDOneClassSVM.html))** class with the parameters: `tol=1e-9`, `nu=0.011`, `learning_rate='adaptive'`, `eta0=0.001`, and `average=False`.

## Results

| Kernel | n_components | Val f1 score (1000 samples) | Test score (1000 samples) 
|---------------------|-----|---------------------|----------------------|
| quantum        | 20   | 0.987        | 0.984                   | 
| quantum          | 100   | 0.9945                | 0.993                   | 
| classical RBF     | 100   | 0.983                 | 0.981                  |
